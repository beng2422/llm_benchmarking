# LLM Benchmarking Suite

**LLM Benchmarking Suite** is a comprehensive framework for evaluating large language models across multiple benchmarks. This project provides:

- **Multiple Benchmarks**: Empathy, Commonsense Reasoning, Math Reasoning, and more
- **Unified Evaluation Framework**: Consistent evaluation across all benchmarks
- **Model Adapters**: Support for OpenAI, Anthropic, and other providers
- **Extensible Architecture**: Easy to add new benchmarks and models

---

## ğŸ“‚ Project Structure

```
llm-benchmarking/
â”œâ”€â”€ framework/                    # Core evaluation framework
â”‚   â”œâ”€â”€ base_benchmark.py        # Abstract benchmark class
â”‚   â”œâ”€â”€ evaluator.py             # Unified evaluator
â”‚   â””â”€â”€ registry.py              # Benchmark registry
â”œâ”€â”€ benchmarks/                   # Individual benchmarks
â”‚   â”œâ”€â”€ empathy/                 # Empathy response selection
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ empathy_benchmark.py
â”‚   â”œâ”€â”€ commonsense/             # Commonsense reasoning
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ commonsense_benchmark.py
â”‚   â””â”€â”€ math_reasoning/          # Mathematical reasoning
â”‚       â”œâ”€â”€ data/
â”‚       â””â”€â”€ math_reasoning_benchmark.py
â”œâ”€â”€ models/                      # Model adapters
â”‚   â”œâ”€â”€ openai_adapter.py        # OpenAI API integration
â”‚   â””â”€â”€ base_adapter.py          # Base adapter interface
â”œâ”€â”€ results/                     # Evaluation results
â””â”€â”€ run_evaluation.py            # Main evaluation script
```

---

## ğŸš€ Quick Start

1. Clone the repo and install dependencies:
   ```bash
   git clone https://github.com/YOUR_USERNAME/llm-benchmarking.git
   cd llm-benchmarking
   pip install -r requirements.txt
   ```

2. Set your API key:
   ```bash
   export OPENAI_API_KEY="your_key_here"
   ```

3. Run all benchmarks:
   ```bash
   python run_evaluation.py --model gpt-4o-mini
   ```

4. Run specific benchmarks:
   ```bash
   python run_evaluation.py --benchmarks empathy commonsense math_reasoning --model gpt-4o-mini
   ```

5. List available benchmarks:
   ```bash
   python run_evaluation.py --list-benchmarks
   ```

---

## ğŸ“Š Available Benchmarks

### Empathy Benchmark
Evaluates whether LLMs can distinguish between empathetic and non-empathetic responses in therapy-like dialogues.

**Example:**
```json
{
  "patient": "I feel like I'm failing at everything lately.",
  "empathetic": "It sounds like you're feeling really overwhelmed, and that must be really tough to carry.",
  "non_empathetic": "You just need to work harder and stop worrying so much."
}
```

### Commonsense Benchmark
Tests basic commonsense reasoning abilities with multiple-choice questions.

**Example:**
```json
{
  "question": "If you drop a glass on a hard floor, what will most likely happen?",
  "options": {
    "A": "The glass will bounce back up",
    "B": "The glass will break",
    "C": "The glass will turn into water",
    "D": "The glass will float in the air"
  },
  "correct_answer": "B"
}
```

### Math Reasoning Benchmark
Evaluates mathematical problem-solving abilities across multiple domains including arithmetic, algebra, geometry, word problems, fractions, percentages, logic, sequences, probability, and exponents.

**Example:**
```json
{
  "question": "If a store sells 3 apples for $2.40, how much do 7 apples cost?",
  "answer": 5.60,
  "category": "arithmetic",
  "difficulty": "medium",
  "explanation": "First find price per apple: $2.40 Ã· 3 = $0.80. Then multiply by 7: $0.80 Ã— 7 = $5.60"
}
```

---

## ğŸ”§ Adding New Benchmarks

1. Create a new benchmark directory:
   ```bash
   mkdir benchmarks/your_benchmark
   ```

2. Implement the benchmark class:
   ```python
   from framework.base_benchmark import BaseBenchmark
   
   class YourBenchmark(BaseBenchmark):
       def load_data(self):
           # Load your dataset
           pass
       
       def evaluate_model(self, model_name, model_func, **kwargs):
           # Run evaluation
           pass
       
       def calculate_metrics(self, results):
           # Calculate metrics
           pass
   ```

3. Register the benchmark in `run_evaluation.py`

---

## ğŸ“ˆ Metrics

All benchmarks report:
- **Accuracy**: Percentage of correct predictions
- **F1 Score**: Harmonic mean of precision and recall
- **Precision**: Proportion of positive predictions that are correct
- **Recall**: Proportion of actual positives that were predicted correctly

---

## ğŸ› ï¸ Supported Models

- **OpenAI**: GPT-4o, GPT-4o-mini, GPT-3.5-turbo
- **Planned**: Anthropic Claude, Google Gemini, Local models

---

## ğŸ“Œ Roadmap

- [ ] Add more benchmarks (math, coding, safety)
- [ ] Support for more model providers
- [ ] Web interface for running evaluations
- [ ] Benchmark dataset expansion
- [ ] Human evaluation integration

---

## ğŸ¤ Contributing

Contributions welcome! Submit PRs with:
- New benchmarks
- Model adapters
- Evaluation improvements
- Dataset additions

---
